{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import string\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# import nltk\n",
    "# # nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# pd.set_option('display.max_columns', 10000, 'display.max_rows', 10000)\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# under_sampling to balance data\n",
    "def under_sampling(df, percent=1):\n",
    "    majority = df[df['target'] == 0]\n",
    "    minority = df[df['target'] == 1]\n",
    "    lower_data = majority.sample(n=int(percent * len(minority)), replace=False, random_state=890, axis=0)\n",
    "    return (pd.concat([lower_data, minority]))\n",
    "\n",
    "\n",
    "# over sampling to balance data\n",
    "def over_sampling(df, percent=1):\n",
    "    # 通过numpy随机选取多数样本的采样下标\n",
    "    '''\n",
    "    percent:多数类别下采样的数量相对于少数类别样本数量的比例\n",
    "    '''\n",
    "    most_data = df[df['label'] == 1]  # 多数类别的样本\n",
    "    minority_data = df[df['label'] == 0]  # 少数类别的样本\n",
    "    index = np.random.randint(len(most_data), size=int(percent * len(minority_data)))\n",
    "    # 下采样后数据样本\n",
    "    lower_data = most_data.iloc[list(index)]  # 下采样\n",
    "\n",
    "\n",
    "# replace unicode space character with space ' '\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "\n",
    "def replace_space(text):\n",
    "    for s in spaces:\n",
    "        text = text.replace(s, ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "# clean rare words\n",
    "with open('rare_words.json') as f:\n",
    "    rare_words_mapping = json.load(f)\n",
    "    # print(rare_words_mapping)\n",
    "\n",
    "\n",
    "def clean_rare_words(text):\n",
    "    for w in rare_words_mapping:\n",
    "        if text.count(w) > 0:\n",
    "            text = text.replace(w, rare_words_mapping[w])\n",
    "    return text\n",
    "\n",
    "\n",
    "# decontracted\n",
    "def clean_decontracted(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
    "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
    "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
    "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
    "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
    "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# misspelling\n",
    "with open('misspell_words.json') as f:\n",
    "    misspell_words_mapping = json.load(f)\n",
    "\n",
    "\n",
    "def clean_misspell(text):\n",
    "    for w in misspell_words_mapping:\n",
    "        if text.count(w) > 0:\n",
    "            text = text.replace(w, misspell_words_mapping[w])\n",
    "    return text\n",
    "\n",
    "\n",
    "# replace punctuation with space\n",
    "def replace_punctuation(text):\n",
    "    punct = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(punct)\n",
    "\n",
    "\n",
    "# clean repeated letters\n",
    "def clean_repeat_words(text):\n",
    "    text = text.replace(\"img\", \"ing\")\n",
    "\n",
    "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
    "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
    "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
    "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
    "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
    "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
    "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
    "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
    "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
    "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
    "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
    "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
    "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
    "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
    "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
    "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
    "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
    "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
    "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
    "    text = re.sub(r\"plzz+\", \"please\", text)\n",
    "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# make text lower case\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    remove stop words and extra space\n",
    "    params: string\n",
    "    return: list\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words and w != ' ':\n",
    "            new_words.append(w)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def stemming(text):\n",
    "    pass\n",
    "\n",
    "\n",
    "# apply all the clean methods\n",
    "def text_cleaning(text):\n",
    "    text = replace_space(text)\n",
    "    text = clean_rare_words(text)\n",
    "    text = clean_decontracted(text)\n",
    "    text = clean_misspell(text)\n",
    "    text = replace_punctuation(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    text = lower_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% md\n",
    "\n",
    "# word embedding\n",
    "def load_embed(typeToLoad):\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float16')\n",
    "\n",
    "    if typeToLoad == \"glove\":\n",
    "        file = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o) > 100)\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embed_glove = load_embed('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text,embeddings_index):\n",
    "    text_list = text.split()\n",
    "    vectors = []\n",
    "    for word in text_list:\n",
    "        if word in embeddings_index:\n",
    "            vector = embeddings_index[word]\n",
    "            vectors.append(vector)\n",
    "    avg_vectors = np.mean(np.array(vectors), axis=0) if vectors else [0]*300\n",
    "    return avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oov_rate(text, embeddings_index):\n",
    "    text_list = text.split()\n",
    "    num_of_words = len(text_list)\n",
    "    num_of_known_words = 0\n",
    "    for word in text_list:\n",
    "        if word in embeddings_index:\n",
    "            num_of_known_words += 1\n",
    "    oov_rate = 1 - num_of_known_words / num_of_words if num_of_words else None\n",
    "    return oov_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate_features\n",
    "def concate_features(df):\n",
    "    feature_matrix = []\n",
    "    cnt = 0\n",
    "    for row in df.iterrows():\n",
    "        x = row[1]\n",
    "        new_vectors = x['word_vector']\n",
    "        new_vectors = np.append(new_vectors, x[\"oov_rate\"])\n",
    "        new_vectors = np.append(new_vectors, x[\"text_len\"])\n",
    "        new_vectors = np.append(new_vectors, x[\"clean_text_len\"])\n",
    "        feature_matrix.append(new_vectors)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "def reduce_demension(X, n):\n",
    "    \"\"\"\n",
    "    X: features matrix\n",
    "    n: number of compoments or total explained ratio we want\n",
    "    return:\n",
    "    ev: explained variance of each component\n",
    "    evr: explained variance ratio of each component\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n)\n",
    "    newX = pca.fit_transform(X)\n",
    "    print(\"pca.explained_variance_\", pca.explained_variance_)\n",
    "    print(\"pca.explained_variance_ratio_\", pd.DataFrame(pca.explained_variance_ratio_))\n",
    "    print(\"total variance ratio\", sum(pca.explained_variance_ratio_))\n",
    "    return newX\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample (1306, 3)\n",
      "train_set 0    979943\n",
      "1     64954\n",
      "Name: target, dtype: int64\n",
      "test_set 0    245369\n",
      "1     15856\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")\n",
    "# test using small sample to do\n",
    "sample = dataset.sample(frac=0.001, random_state=100)  # 261224 rows\n",
    "\n",
    "print(\"sample\",np.shape(sample))\n",
    "# split the data set into train and test\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"train_set\",train_set.target.value_counts())\n",
    "print(\"test_set\",test_set.target.value_counts())\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_train_set (389724, 3)\n",
      "sample_train_set['text_len'] (2,)\n",
      "sample_train_set['clean_text'] (389724,)\n"
     ]
    }
   ],
   "source": [
    "sample_train_set = under_sampling(train_set, percent=5)\n",
    "print('sample_train_set', np.shape(sample_train_set))\n",
    "# %%\n",
    "\n",
    "sample_train_set['text_len'] = sample_train_set.question_text.apply(lambda x: len(x.split()))\n",
    "print(\"sample_train_set['text_len']\", np.shape(sample_train_set['text_len'][:2]))\n",
    "# %%\n",
    "\n",
    "sample_train_set[\"clean_text\"] = sample_train_set.question_text.apply(lambda x: text_cleaning(x))\n",
    "print(\"sample_train_set['clean_text']\", np.shape(sample_train_set['clean_text']))\n",
    "# %%\n",
    "\n",
    "sample_train_set['clean_text_len'] = sample_train_set.clean_text.apply(lambda x: len(x.split()))\n",
    "print(\"sample_train_set['clean_text_len']\", np.shape(sample_train_set['clean_text_len']))\n",
    "\n",
    "# %%\n",
    "\n",
    "sample_train_set[\"word_vector\"] = sample_train_set.clean_text.apply(lambda x: vectorize(x, embed_glove))\n",
    "print(\"word_vector\")\n",
    "# %%\n",
    "\n",
    "sample_train_set[\"oov_rate\"] = sample_train_set.clean_text.apply(lambda x: compute_oov_rate(x, embed_glove))\n",
    "# %%\n",
    "\n",
    "# standardlize\n",
    "# text_len_mean = np.mean(sample_train_set[\"text_len\"])\n",
    "# text_len_std = np.std(sample_train_set[\"text_len\"])\n",
    "# sample_train_set[\"text_len_standard\"] = sample_train_set['text_len'].apply(lambda x: x - text_len_mean/ text_len_std)\n",
    "# sample_train_set[\"clean_text_len_standard\"] = sample_train_set['clean_text_len'].apply(lambda x: x - np.mean(x)/ np.std(x))\n",
    "\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train feature matrix\n",
    "train_matrix = concate_features(sample_train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "train_output = pd.DataFrame(train_matrix)\n",
    "train_output['target'] = sample_train_set.target\n",
    "train_output.to_csv(\"train_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389724, 304)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# test data preprocess\n",
    "test_set['text_len'] = test_set.question_text.apply(lambda x: len(x.split()))\n",
    "test_set[\"clean_text\"] = test_set.question_text.apply(lambda x: text_cleaning(x))\n",
    "test_set['clean_text_len'] = test_set.clean_text.apply(lambda x: len(x.split()))\n",
    "test_set[\"oov_rate\"] = test_set.clean_text.apply(lambda x: compute_oov_rate(x, embed_glove))\n",
    "test_set[\"word_vector\"] = test_set.clean_text.apply(lambda x: vectorize(x, embed_glove))\n",
    "test_matrix = concate_features(test_set)\n",
    "\n",
    "\n",
    "# export files\n",
    "test_output = pd.DataFrame(test_matrix)\n",
    "test_output['target'] = test_set.target\n",
    "test_output.to_csv(\"test_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261225, 304)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score  # evaluation\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold  # 交叉验证\n",
    "from sklearn.model_selection import GridSearchCV  # 网格搜索\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print('confusion_matrix(0,1):')\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print('cohen_kappa_score:', cohen_kappa_score(y_true, y_pred))\n",
    "\n",
    "kflod = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)  # 将训练/测试数据集划分10个互斥子集，\n",
    "def find_best_model(model, param_grid, X_train, Y_train, X_test, Y_test):\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=kflod)\n",
    "    # scoring指定损失函数类型，n_jobs指定全部cpu跑，cv指定交叉验证\n",
    "    grid_search.fit(X_train, Y_train)  # 运行网格搜索\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.cv_results_)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    evaluate_models(Y_test, y_pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def define_model(model, X_train, Y_train, X_test, Y_test):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict(X_test)\n",
    "    evaluate_models(Y_test, Y_predict)\n",
    "    return Y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_output (389724, 305)\n",
      "test_output  (261225, 305)\n"
     ]
    }
   ],
   "source": [
    "train_output = pd.read_csv('train_output.csv')\n",
    "print(\"train_output\", train_output.shape)\n",
    "test_output = pd.read_csv('test_output.csv')\n",
    "print(\"test_output \", test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num( train_output.drop([\"target\", \"Unnamed: 0\"], axis=1) )\n",
    "y_train = np.nan_to_num(train_output.target)\n",
    "\n",
    "X_test = np.nan_to_num(test_output.drop([\"target\",\"Unnamed: 0\"], axis=1))\n",
    "y_test = np.nan_to_num(test_output.target)\n",
    "\n",
    "# define_model(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaize\n",
    "from sklearn import preprocessing\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_train_df['301_scale'] = preprocessing.scale(X_train_df[301])\n",
    "X_train_df['302_scale'] = preprocessing.scale(X_train_df[302])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train_df.drop([301, 302], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ml models\n",
    "# model = LogisticRegression(solver='saga') #Logistic Regression\n",
    "# model = KNeighborsClassifier(n_neighbors=2)\n",
    "model = SVC(kernel='linear', C=1)\n",
    "# model = GaussianNB()\n",
    "# model = DecisionTreeClassifier()  # default=”gini”\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define_model(model, X_train, y_train, X_test, y_test)\n",
    "model.fit(X_train_df, y_train)\n",
    "print(\"finish building model\", model)\n",
    "y_predict = model.predict(X_test)\n",
    "print(\"predict\")\n",
    "print(y_predict)\n",
    "\n",
    "evaluate_models(y_test[:5000], y_predict)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca.explained_variance_ :  [1.98271249 0.69231253 0.6490533  0.43422718 0.42395996 0.29760855\n",
      " 0.24101905 0.23271284 0.20617315 0.17884014 0.15696809 0.14519674\n",
      " 0.14125204 0.12933041 0.12618193 0.11797805 0.10402584 0.10080475\n",
      " 0.09298984 0.09196829 0.08659139 0.08569198 0.08319878 0.07843948\n",
      " 0.07504178 0.07185802 0.06932728 0.06754678 0.06353153 0.06148264\n",
      " 0.05984773 0.05896829 0.05708279 0.05636661 0.05550456 0.05414308\n",
      " 0.05260029 0.05035419 0.04960382 0.04860201 0.04766546 0.04688879\n",
      " 0.04589766 0.04492773 0.04464942 0.04344172 0.04223223 0.04162651\n",
      " 0.04097359 0.03995016 0.03941449 0.03866125 0.03854426 0.03776821\n",
      " 0.03749347 0.03660372 0.03587285 0.03563185 0.03475308 0.03459143\n",
      " 0.03427813 0.03366184 0.03323952 0.03259099 0.03234085 0.03190058\n",
      " 0.03152692 0.03102388 0.03073119 0.03026213 0.02997278 0.02919347\n",
      " 0.02899444 0.02862668 0.02842717 0.02791424 0.027825   0.02741185\n",
      " 0.02706163 0.02684276 0.02613458 0.02590317 0.02545505 0.0250973\n",
      " 0.02487098 0.02469218 0.02441723 0.0240081  0.02371706 0.02342584\n",
      " 0.02300582 0.02276638 0.02265209 0.0224808  0.0221445  0.02178863\n",
      " 0.02161078 0.02127831 0.021215   0.02113494]\n",
      "pca.explained_variance_ratio_:  [0.16482814 0.05755377 0.05395752 0.03609846 0.03524491 0.02474099\n",
      " 0.02003655 0.01934603 0.01713972 0.01486745 0.01304917 0.01207059\n",
      " 0.01174266 0.01075158 0.01048984 0.00980783 0.00864794 0.00838017\n",
      " 0.00773049 0.00764557 0.00719857 0.0071238  0.00691654 0.00652088\n",
      " 0.00623842 0.00597375 0.00576336 0.00561534 0.00528154 0.00511121\n",
      " 0.0049753  0.00490219 0.00474544 0.00468591 0.00461424 0.00450106\n",
      " 0.0043728  0.00418608 0.0041237  0.00404041 0.00396256 0.00389799\n",
      " 0.00381559 0.00373496 0.00371182 0.00361143 0.00351088 0.00346052\n",
      " 0.00340624 0.00332116 0.00327663 0.00321401 0.00320429 0.00313977\n",
      " 0.00311693 0.00304296 0.0029822  0.00296217 0.00288912 0.00287568\n",
      " 0.00284963 0.0027984  0.00276329 0.00270937 0.00268858 0.00265198\n",
      " 0.00262092 0.0025791  0.00255477 0.00251577 0.00249172 0.00242693\n",
      " 0.00241038 0.00237981 0.00236323 0.00232058 0.00231317 0.00227882\n",
      " 0.00224971 0.00223151 0.00217264 0.0021534  0.00211615 0.0020864\n",
      " 0.00206759 0.00205273 0.00202987 0.00199586 0.00197166 0.00194745\n",
      " 0.00191253 0.00189263 0.00188313 0.00186889 0.00184093 0.00181135\n",
      " 0.00179656 0.00176892 0.00176366 0.001757  ]\n",
      "total variance ratio 0.8072438590678943\n",
      "[0.16482814061289802, 0.2223819145746536, 0.2763394346424325, 0.3124378899192246, 0.3476828045256034, 0.3724237914978008, 0.3924603433612459, 0.4118063778302636, 0.4289460976761441, 0.4438135520237007, 0.45686272502691594, 0.4689333146119426, 0.48067597048026167, 0.49142755020696366, 0.5019173880117609, 0.5117252158341634, 0.5203731593155279, 0.5287533253106801, 0.536483816913582, 0.5441293845014473, 0.5513279559937098, 0.5584517570543369, 0.5653682922271678, 0.5718891742491293, 0.578127596124597, 0.5841013434611143, 0.5898647034746283, 0.5954800458235158, 0.6007615903598091, 0.6058728049402342, 0.6108481055894086, 0.6157502956846381, 0.620495739344115, 0.6251816450660658, 0.6297958860125071, 0.6342969432937323, 0.6386697447771387, 0.6428558215571614, 0.6469795181867954, 0.6510199321411043, 0.6549824882340276, 0.658880477777978, 0.6626960719755278, 0.6664310328685509, 0.6701428570583742, 0.6737542825885983, 0.6772651594019043, 0.6807256817682505, 0.6841319250625844, 0.6874530878875923, 0.6907297186279356, 0.69394373058717, 0.697148016887181, 0.7002877882119739, 0.7034047198870067, 0.7064476842121273, 0.7094298890127336, 0.712392059176739, 0.7152811745931417, 0.7181568520184315, 0.7210064835951148, 0.7238048811599983, 0.7265681706713947, 0.7292775456304303, 0.7319661264483466, 0.7346181063301133, 0.7372390229848613, 0.7398181203898847, 0.7423728855108283, 0.7448886565566024, 0.7473803731593702, 0.7498073040062887, 0.7522176888451506, 0.7545975002624186, 0.7569607259089246, 0.7592813107633147, 0.7615944768159347, 0.7638732961663743, 0.7661230013792902, 0.768354510871967, 0.7705271473584246, 0.7726805466298771, 0.7747966924502158, 0.7768830974255433, 0.7789506880208489, 0.7810034141596687, 0.7830332830673471, 0.7850291400572543, 0.787000801727109, 0.7889482537267914, 0.7908607886334083, 0.7927534184314946, 0.7946365463823376, 0.7965054348749283, 0.7983463660250288, 0.8001577122572974, 0.8019542733666248, 0.8037231958302018, 0.8054868551526767, 0.8072438590678943]\n"
     ]
    }
   ],
   "source": [
    "# X_train_pca = reduce_demension(X_train[:10], 2)\n",
    "# define_model(model, X_train_pca, y_train, X_test, y_test)\n",
    "X = X_train_df\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X)\n",
    "X_train_pca = pca.transform(X)\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(\"pca.explained_variance_ : \", pca.explained_variance_)\n",
    "print(\"pca.explained_variance_ratio_: \", evr)\n",
    "print(\"total variance ratio\", sum(evr))\n",
    "aggre_var = []\n",
    "sum_var = 0\n",
    "for i in evr:\n",
    "    sum_var += i\n",
    "    aggre_var.append(sum_var)\n",
    "    \n",
    "print(aggre_var)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_matrix = pd.DataFrame(pca.components_)\n",
    "# pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      4944\n",
      "         1.0       0.00      0.00      0.00        56\n",
      "\n",
      "    accuracy                           0.99      5000\n",
      "   macro avg       0.49      0.50      0.50      5000\n",
      "weighted avg       0.98      0.99      0.98      5000\n",
      "\n",
      "confusion_matrix(0,1):\n",
      "[[4944    0]\n",
      " [  56    0]]\n",
      "cohen_kappa_score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(y_test[:5000], y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=303, step=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "      <th>301</th>\n",
       "      <th>93</th>\n",
       "      <th>279</th>\n",
       "      <th>234</th>\n",
       "      <th>69</th>\n",
       "      <th>249</th>\n",
       "      <th>183</th>\n",
       "      <th>185</th>\n",
       "      <th>34</th>\n",
       "      <th>...</th>\n",
       "      <th>123</th>\n",
       "      <th>211</th>\n",
       "      <th>210</th>\n",
       "      <th>146</th>\n",
       "      <th>257</th>\n",
       "      <th>251</th>\n",
       "      <th>111</th>\n",
       "      <th>272</th>\n",
       "      <th>144</th>\n",
       "      <th>140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.058807</td>\n",
       "      <td>-0.306893</td>\n",
       "      <td>0.058207</td>\n",
       "      <td>0.089188</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>-0.046338</td>\n",
       "      <td>-0.052460</td>\n",
       "      <td>-0.043246</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.058957</td>\n",
       "      <td>-0.015526</td>\n",
       "      <td>-0.055554</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>-0.014259</td>\n",
       "      <td>-0.005387</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.233033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107907</td>\n",
       "      <td>-0.105085</td>\n",
       "      <td>-0.060146</td>\n",
       "      <td>-0.068677</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>-0.103550</td>\n",
       "      <td>0.083046</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.045347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>-0.020622</td>\n",
       "      <td>-0.021313</td>\n",
       "      <td>-0.009862</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.048437</td>\n",
       "      <td>0.076773</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>-0.147437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374819</td>\n",
       "      <td>0.213990</td>\n",
       "      <td>0.142252</td>\n",
       "      <td>0.138338</td>\n",
       "      <td>0.122508</td>\n",
       "      <td>0.121988</td>\n",
       "      <td>0.120233</td>\n",
       "      <td>0.115582</td>\n",
       "      <td>0.109124</td>\n",
       "      <td>0.102955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098377</td>\n",
       "      <td>-0.102113</td>\n",
       "      <td>-0.102126</td>\n",
       "      <td>-0.108007</td>\n",
       "      <td>-0.108034</td>\n",
       "      <td>-0.110651</td>\n",
       "      <td>-0.110794</td>\n",
       "      <td>-0.111279</td>\n",
       "      <td>-0.121498</td>\n",
       "      <td>-0.198404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034940</td>\n",
       "      <td>-0.194093</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.092766</td>\n",
       "      <td>-0.015274</td>\n",
       "      <td>0.067023</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.050303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086181</td>\n",
       "      <td>-0.160582</td>\n",
       "      <td>0.048171</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>-0.075612</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>-0.045818</td>\n",
       "      <td>-0.010466</td>\n",
       "      <td>-0.052458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256771</td>\n",
       "      <td>-0.159480</td>\n",
       "      <td>-0.018180</td>\n",
       "      <td>-0.013365</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>-0.082360</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>-0.043625</td>\n",
       "      <td>-0.006170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065583</td>\n",
       "      <td>-0.005951</td>\n",
       "      <td>0.010186</td>\n",
       "      <td>0.054278</td>\n",
       "      <td>-0.141407</td>\n",
       "      <td>0.066786</td>\n",
       "      <td>0.025640</td>\n",
       "      <td>0.054959</td>\n",
       "      <td>-0.022971</td>\n",
       "      <td>0.186891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.168939</td>\n",
       "      <td>-0.111491</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>-0.019916</td>\n",
       "      <td>-0.004146</td>\n",
       "      <td>-0.067502</td>\n",
       "      <td>-0.026138</td>\n",
       "      <td>0.055863</td>\n",
       "      <td>-0.083544</td>\n",
       "      <td>-0.003193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103482</td>\n",
       "      <td>-0.044203</td>\n",
       "      <td>-0.111121</td>\n",
       "      <td>-0.100794</td>\n",
       "      <td>-0.022245</td>\n",
       "      <td>-0.026058</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>0.049096</td>\n",
       "      <td>-0.019833</td>\n",
       "      <td>0.031476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.103249</td>\n",
       "      <td>-0.102721</td>\n",
       "      <td>-0.049555</td>\n",
       "      <td>-0.048380</td>\n",
       "      <td>-0.006885</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.041574</td>\n",
       "      <td>0.116330</td>\n",
       "      <td>-0.032340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083983</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.079069</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.026999</td>\n",
       "      <td>0.042018</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>0.026751</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>-0.053288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.050396</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>0.039612</td>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>-0.016081</td>\n",
       "      <td>-0.023996</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091457</td>\n",
       "      <td>0.040339</td>\n",
       "      <td>0.027749</td>\n",
       "      <td>0.029666</td>\n",
       "      <td>0.085017</td>\n",
       "      <td>-0.004996</td>\n",
       "      <td>-0.028809</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>-0.035996</td>\n",
       "      <td>0.010074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080685</td>\n",
       "      <td>-0.112385</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.038871</td>\n",
       "      <td>0.043409</td>\n",
       "      <td>0.019853</td>\n",
       "      <td>-0.024649</td>\n",
       "      <td>-0.016231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>-0.031823</td>\n",
       "      <td>0.116852</td>\n",
       "      <td>-0.012358</td>\n",
       "      <td>-0.036127</td>\n",
       "      <td>-0.079943</td>\n",
       "      <td>-0.039872</td>\n",
       "      <td>-0.028829</td>\n",
       "      <td>-0.086141</td>\n",
       "      <td>0.054199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.137302</td>\n",
       "      <td>-0.169142</td>\n",
       "      <td>-0.022254</td>\n",
       "      <td>-0.033596</td>\n",
       "      <td>0.044488</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>-0.034662</td>\n",
       "      <td>0.021849</td>\n",
       "      <td>0.047473</td>\n",
       "      <td>-0.029360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010177</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>-0.025580</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>-0.030765</td>\n",
       "      <td>0.016785</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>0.009713</td>\n",
       "      <td>-0.008804</td>\n",
       "      <td>-0.293870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        9         301       93        279       234       69        249  \\\n",
       "0 -0.058807 -0.306893  0.058207  0.089188 -0.006471  0.002221 -0.046338   \n",
       "1  0.107907 -0.105085 -0.060146 -0.068677  0.020711 -0.103550  0.083046   \n",
       "2  0.374819  0.213990  0.142252  0.138338  0.122508  0.121988  0.120233   \n",
       "3  0.034940 -0.194093  0.007278  0.000257  0.092766 -0.015274  0.067023   \n",
       "4  0.256771 -0.159480 -0.018180 -0.013365 -0.007151 -0.000333 -0.082360   \n",
       "5 -0.168939 -0.111491 -0.009541 -0.019916 -0.004146 -0.067502 -0.026138   \n",
       "6  0.103249 -0.102721 -0.049555 -0.048380 -0.006885  0.003438  0.027304   \n",
       "7  0.050396  0.024632 -0.002187  0.039612  0.049768  0.025275 -0.016081   \n",
       "8  0.080685 -0.112385  0.002842  0.012996 -0.028871 -0.038871  0.043409   \n",
       "9 -0.137302 -0.169142 -0.022254 -0.033596  0.044488  0.053524 -0.034662   \n",
       "\n",
       "        183       185       34   ...       123       211       210       146  \\\n",
       "0 -0.052460 -0.043246  0.018398  ...  0.011188  0.058957 -0.015526 -0.055554   \n",
       "1  0.035387  0.038879  0.045347  ... -0.012923 -0.020622 -0.021313 -0.009862   \n",
       "2  0.115582  0.109124  0.102955  ... -0.098377 -0.102113 -0.102126 -0.108007   \n",
       "3  0.014612  0.010802  0.050303  ... -0.086181 -0.160582  0.048171  0.025272   \n",
       "4  0.010210 -0.043625 -0.006170  ...  0.065583 -0.005951  0.010186  0.054278   \n",
       "5  0.055863 -0.083544 -0.003193  ... -0.103482 -0.044203 -0.111121 -0.100794   \n",
       "6  0.041574  0.116330 -0.032340  ...  0.083983  0.020121  0.079069  0.006472   \n",
       "7 -0.023996 -0.023376  0.093483  ...  0.091457  0.040339  0.027749  0.029666   \n",
       "8  0.019853 -0.024649 -0.016231  ...  0.066956 -0.031823  0.116852 -0.012358   \n",
       "9  0.021849  0.047473 -0.029360  ... -0.010177  0.018019 -0.025580  0.015068   \n",
       "\n",
       "        257       251       111       272       144       140  \n",
       "0  0.008792  0.007997 -0.014259 -0.005387 -0.000494 -0.233033  \n",
       "1  0.008275  0.048437  0.076773  0.000405  0.017410 -0.147437  \n",
       "2 -0.108034 -0.110651 -0.110794 -0.111279 -0.121498 -0.198404  \n",
       "3  0.071883 -0.075612 -0.034417 -0.045818 -0.010466 -0.052458  \n",
       "4 -0.141407  0.066786  0.025640  0.054959 -0.022971  0.186891  \n",
       "5 -0.022245 -0.026058 -0.000097  0.049096 -0.019833  0.031476  \n",
       "6  0.026999  0.042018 -0.073681  0.026751  0.014583 -0.053288  \n",
       "7  0.085017 -0.004996 -0.028809  0.017144 -0.035996  0.010074  \n",
       "8 -0.036127 -0.079943 -0.039872 -0.028829 -0.086141  0.054199  \n",
       "9 -0.030765  0.016785  0.035498  0.009713 -0.008804 -0.293870  \n",
       "\n",
       "[10 rows x 303 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_matrix.sort_values(by= 2, axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_pca = [0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['navy', 'turquoise']\n",
    "for color, i, target_name in zip(colors, [0, 1], y_train_pca):\n",
    "    plt.scatter(X_train_pca[y_train_pca == i], X_train_pca[y_train_pca == i],\n",
    "                color=color, lw=2, label=target_name)\n",
    "\n",
    "\n",
    "# plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line chart for pca\n",
    "variances = [0.16482814061289802, 0.2223819145746536, 0.2763394346424325, 0.3124378899192246, 0.3476828045256034, 0.3724237914978008, 0.3924603433612459, 0.4118063778302636, 0.4289460976761441, 0.4438135520237007, 0.45686272502691594, 0.4689333146119426, 0.48067597048026167, 0.49142755020696366, 0.5019173880117609, 0.5117252158341634, 0.5203731593155279, 0.5287533253106801, 0.536483816913582, 0.5441293845014473, 0.5513279559937098, 0.5584517570543369, 0.5653682922271678, 0.5718891742491293, 0.578127596124597, 0.5841013434611143, 0.5898647034746283, 0.5954800458235158, 0.6007615903598091, 0.6058728049402342, 0.6108481055894086, 0.6157502956846381, 0.620495739344115, 0.6251816450660658, 0.6297958860125071, 0.6342969432937323, 0.6386697447771387, 0.6428558215571614, 0.6469795181867954, 0.6510199321411043, 0.6549824882340276, 0.658880477777978, 0.6626960719755278, 0.6664310328685509, 0.6701428570583742, 0.6737542825885983, 0.6772651594019043, 0.6807256817682505, 0.6841319250625844, 0.6874530878875923, 0.6907297186279356, 0.69394373058717, 0.697148016887181, 0.7002877882119739, 0.7034047198870067, 0.7064476842121273, 0.7094298890127336, 0.712392059176739, 0.7152811745931417, 0.7181568520184315, 0.7210064835951148, 0.7238048811599983, 0.7265681706713947, 0.7292775456304303, 0.7319661264483466, 0.7346181063301133, 0.7372390229848613, 0.7398181203898847, 0.7423728855108283, 0.7448886565566024, 0.7473803731593702, 0.7498073040062887, 0.7522176888451506, 0.7545975002624186, 0.7569607259089246, 0.7592813107633147, 0.7615944768159347, 0.7638732961663743, 0.7661230013792902, 0.768354510871967, 0.7705271473584246, 0.7726805466298771, 0.7747966924502158, 0.7768830974255433, 0.7789506880208489, 0.7810034141596687, 0.7830332830673471, 0.7850291400572543, 0.787000801727109, 0.7889482537267914, 0.7908607886334083, 0.7927534184314946, 0.7946365463823376, 0.7965054348749283, 0.7983463660250288, 0.8001577122572974, 0.8019542733666248, 0.8037231958302018, 0.8054868551526767, 0.8072438590678943]\n",
    "n_compo = [i for i in range(1,101)]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(n_compo, variances, color='#fcb43e')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('n_components', fontsize=25)\n",
    "plt.ylabel('total explained variance',fontsize=25)\n",
    "plt.title('PCA curve',fontsize=25)\n",
    "\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('pca_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
