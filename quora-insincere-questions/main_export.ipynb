{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "main_export.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGHHFp2NWL8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a594df54-c5e7-405a-f93f-da223e2564f0"
      },
      "source": [
        "!mv kaggle.json /root/.kaggle/kaggle.json"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgm76AzEWdcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "a8c5453f-51d4-49a0-ba88-ea32e490e93c"
      },
      "source": [
        "!kaggle competitions download -c quora-insincere-questions-classification"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            " 76% 42.0M/54.9M [00:00<00:00, 49.3MB/s]\n",
            "100% 54.9M/54.9M [00:00<00:00, 86.5MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/4.09M [00:00<?, ?B/s]\n",
            "100% 4.09M/4.09M [00:00<00:00, 67.0MB/s]\n",
            "Downloading embeddings.zip to /content\n",
            "100% 5.95G/5.96G [01:26<00:00, 54.2MB/s]\n",
            "100% 5.96G/5.96G [01:26<00:00, 74.1MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 44% 7.00M/15.8M [00:00<00:00, 40.1MB/s]\n",
            "100% 15.8M/15.8M [00:00<00:00, 62.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO4QYXMqmT8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "753e0a3c-2562-4c2f-c5ae-2d1da551f29e"
      },
      "source": [
        "!unzip embeddings.zip -d embeddings"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  embeddings.zip\n",
            "   creating: embeddings/GoogleNews-vectors-negative300/\n",
            "   creating: embeddings/glove.840B.300d/\n",
            "   creating: embeddings/paragram_300_sl999/\n",
            "   creating: embeddings/wiki-news-300d-1M/\n",
            "  inflating: embeddings/glove.840B.300d/glove.840B.300d.txt  \n",
            "  inflating: embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin  \n",
            "  inflating: embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec  \n",
            "  inflating: embeddings/paragram_300_sl999/README.txt  \n",
            "  inflating: embeddings/paragram_300_sl999/paragram_300_sl999.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVjOUzV2Ecrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "56d9ec8a-d878-44cd-eafc-678ea1fd29c4"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1utr61xnGEG-",
        "colab_type": "text"
      },
      "source": [
        "### Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1dxUSViGu8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def under_sampling(df, percent=1, random_seed=11):\n",
        "    majority = df[df['target'] == 0]\n",
        "    minority = df[df['target'] == 1]\n",
        "    lower_data = majority.sample(n=int(percent * len(minority)), replace=False, random_state=random_seed, axis=0)\n",
        "    return (pd.concat([lower_data, minority]))\n",
        "\n",
        "def statistic_features(df_X, embedding):\n",
        "  df_X[\"text_len\"] = df_X[\"question_text\"].apply(lambda x: len(x.split()))\n",
        "  df_X[\"clean_text\"] = df_X[\"question_text\"].apply(lambda x: text_cleaning(x))\n",
        "  df_X['clean_text_len'] = df_X[\"clean_text\"].apply(lambda x: len(x.split()))\n",
        "  df_X[\"oov_rate\"] = df_X[\"clean_text\"].apply(lambda x: compute_oov_rate(x, embedding))\n",
        "  return df_X\n",
        "\n",
        "def concate_features(df):\n",
        "    feature_matrix = []\n",
        "    for row in df.iterrows():\n",
        "        x = row[1]\n",
        "        new_vectors = x['word_vector']\n",
        "        new_vectors = np.append(new_vectors, x[\"oov_rate\"])\n",
        "        new_vectors = np.append(new_vectors, x[\"text_len\"])\n",
        "        new_vectors = np.append(new_vectors, x[\"clean_text_len\"])\n",
        "        feature_matrix.append(new_vectors)\n",
        "    return pd.DataFrame(feature_matrix)\n",
        "\n",
        "\n",
        "def vectorize(text, embeddings_index, max_len=20):\n",
        "    text_list = text.split()\n",
        "    init_vector = [0]*300\n",
        "    if len(text_list) >= max_len:\n",
        "        text_list = text_list[:max_len]\n",
        "    else:\n",
        "        pad_len = max_len - len(text_list)\n",
        "        for _ in range(pad_len):\n",
        "            text_list.append(\"<PAD>\")\n",
        "    vectors = []\n",
        "    for word in text_list:\n",
        "        if word == \"<PAD>\":\n",
        "            vectors.append(init_vector)\n",
        "        elif word in embeddings_index:\n",
        "            vectors.append(embeddings_index[word])\n",
        "        else:\n",
        "            vectors.append(init_vector)\n",
        "    vectors = np.mean(vectors, axis=0)\n",
        "    return vectors\n",
        "\n",
        "def compute_oov_rate(text, embeddings_index):\n",
        "    text_list = text.split()\n",
        "    num_of_words = len(text_list)\n",
        "    num_of_known_words = 0\n",
        "    for word in text_list:\n",
        "        if word in embeddings_index:\n",
        "            num_of_known_words += 1\n",
        "    oov_rate = 1 - num_of_known_words / num_of_words if num_of_words else None\n",
        "    return oov_rate"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l5akddUGwvl",
        "colab_type": "text"
      },
      "source": [
        "### Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II2x5Fi8Ecrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b034e357-e3c5-46e9-9eec-002b2c915de3"
      },
      "source": [
        "%%time\n",
        "# replace unicode space character with space ' '\n",
        "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
        "def replace_space(text):\n",
        "    for s in spaces:\n",
        "        text = text.replace(s, ' ')\n",
        "    return text\n",
        "\n",
        "# clean rare words\n",
        "# with open('rare_words.json') as f:\n",
        "#     rare_words_mapping = json.load(f)\n",
        "def clean_rare_words(text):\n",
        "    for w in rare_words_mapping:\n",
        "        if text.count(w) > 0:\n",
        "            text = text.replace(w, rare_words_mapping[w])\n",
        "    return text\n",
        "\n",
        "def clean_decontracted(text):\n",
        "    # specific\n",
        "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
        "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
        "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
        "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
        "\n",
        "    # general\n",
        "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
        "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
        "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# with open('misspell_words.json') as f:\n",
        "#     misspell_words_mapping = json.load(f)\n",
        "def clean_misspell(text):\n",
        "    for w in misspell_words_mapping:\n",
        "        if text.count(w) > 0:\n",
        "            text = text.replace(w, misspell_words_mapping[w])\n",
        "    return text\n",
        "\n",
        "\n",
        "# replace punctuation with space\n",
        "def replace_punctuation(text):\n",
        "    punct = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(punct)\n",
        "\n",
        "\n",
        "# clean repeated letters\n",
        "def clean_repeat_words(text):\n",
        "    text = text.replace(\"img\", \"ing\")\n",
        "\n",
        "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
        "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
        "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
        "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
        "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
        "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
        "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
        "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
        "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
        "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
        "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
        "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
        "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
        "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
        "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
        "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
        "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
        "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
        "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
        "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
        "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
        "    text = re.sub(r\"plzz+\", \"please\", text)\n",
        "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def lower_words(text):\n",
        "    return text.lower()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"\n",
        "    remove stop words and extra space\n",
        "    params: string\n",
        "    return: list\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for w in words:\n",
        "        if w not in stop_words and w != ' ':\n",
        "            new_words.append(w)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "\n",
        "def stemming(text):\n",
        "    pass\n",
        "\n",
        "# apply all the clean methods\n",
        "def text_cleaning(text):\n",
        "    text = replace_space(text)\n",
        "    #text = clean_rare_words(text)\n",
        "    text = clean_decontracted(text)\n",
        "    #text = clean_misspell(text)\n",
        "    text = replace_punctuation(text)\n",
        "    text = clean_repeat_words(text)\n",
        "    text = lower_words(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.46 ms, sys: 0 ns, total: 1.46 ms\n",
            "Wall time: 1.47 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ak9wKbyHHw5",
        "colab_type": "text"
      },
      "source": [
        "### Word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJ7331aEcr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embed(typeToLoad):\n",
        "    def get_coefs(word, *arr):\n",
        "        return word, np.asarray(arr, dtype='float16')\n",
        "\n",
        "    if typeToLoad == \"glove\":\n",
        "        file = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o) > 100)\n",
        "    elif typeToLoad == \"word2vec\":\n",
        "        # file = 'embeddings⁩/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin⁩'\n",
        "        file = 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
        "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)  # query word vector from the file\n",
        "    elif typeToLoad == \"fasttext\":\n",
        "        # file = \"⁨embeddings⁩/wiki-news-300d-1M⁩/wiki-news-300d-1M.vec\"\n",
        "        file = 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
        "\n",
        "    return embeddings_index"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD_Iq5IVKXIf",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls_92Tu2Ecs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.model_selection import GridSearchCV "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwbFJpk8EctD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_models(y_true, y_pred):\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    cks = cohen_kappa_score(y_true, y_pred)\n",
        "    print(\"f1_score\", f1)\n",
        "    print('cohen_kappa_score:', cks)\n",
        "    return f1, cks\n",
        "\n",
        "def find_best_model(model, param_grid, X_train, Y_train, kfold):\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=kfold)# scoring指定损失函数类型，n_jobs指定全部cpu跑，cv指定交叉验证\n",
        "    grid_search.fit(X_train, Y_train)  \n",
        "    print(\"best_params_\", grid_search.best_params_)\n",
        "    print(\"best_score_\", grid_search.best_score_)\n",
        "    print(\"cv_results_\", grid_search.cv_results_)\n",
        "    return grid_search\n",
        "\n",
        "def build_cv_model(model, train_x, y, kfold):\n",
        "    k = 0\n",
        "    results = []\n",
        "    for train_index, test_index in kfold.split(train_x, y):\n",
        "        k += 1\n",
        "        print(\"--- cv\", k)\n",
        "        X_train, y_train = train_x.iloc[train_index], y[train_index]\n",
        "        X_test, y_test = train_x.iloc[test_index], y[test_index]\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        f1, cks = evaluate_models(y_test, y_pred)\n",
        "        results.append([f1,cks])\n",
        "    avg_f1, avg_cks = np.array(results).mean(axis=0) \n",
        "    print(\"Average cv score, f1 {}, cohen_kappa_score {}\".format(avg_f1, avg_cks))\n",
        "    return results\n",
        "\n",
        "# PCA\n",
        "def reduce_demension(X, n):\n",
        "    pca = PCA(n_components=n)\n",
        "    newX = pca.fit_transform(X)\n",
        "    print(\"pca.explained_variance_\", pca.explained_variance_)\n",
        "    print(\"pca.explained_variance_ratio_\", pd.DataFrame(pca.explained_variance_ratio_))\n",
        "    print(\"total variance ratio\", sum(pca.explained_variance_ratio_))\n",
        "    return newX"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6XM4vyvKgys",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd3dzS-a1lEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d39c20da-fc39-4965-bbda-70da1a38bfd4"
      },
      "source": [
        "print(\"Load embedding\")\n",
        "embedding = load_embed(\"fasttext\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load embedding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtGhz39sKdBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "383d763f-0c53-463b-b366-e067f03cfe39"
      },
      "source": [
        "data_path = \"./\"\n",
        "print(\"Load training data\")\n",
        "train = pd.read_csv(data_path +\"train.csv.zip\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylfRbGxemSRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a1bca7b9-e246-459c-f5f6-a33b4dc7a5c1"
      },
      "source": [
        "X, y = train[\"question_text\"], train[\"target\"]\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "sample_train = under_sampling(pd.concat([train_x, train_y], axis=1), percent=2, random_seed=11)\n",
        "#sample_train = train.sample(frac=0.1, random_state=100)  # 261224 rows\n",
        "print(\"sample size\", sample_train.shape)\n",
        "\n",
        "sample_train.reset_index(drop=True, inplace=True)\n",
        "train_x, y = sample_train[\"question_text\"], sample_train[\"target\"]\n",
        "\n",
        "print(\"Text cleaning\")\n",
        "train_x = train_x.apply(text_cleaning)\n",
        "\n",
        "print(\"Text vectorizing\")\n",
        "train_x = train_x.apply(lambda text: vectorize(text, embedding, max_len=20))\n",
        "train_x = pd.DataFrame(train_x.array)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample size (218835, 2)\n",
            "Text cleaning\n",
            "Text vectorizing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rn-CdkYz2bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess testing data\n",
        "test_x = test_x.apply(text_cleaning)\n",
        "test_x = test_x.apply(lambda text: vectorize(text, embedding, max_len=20))\n",
        "test_x = pd.DataFrame(test_x.array)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKoNXdc9Okjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "63390448-5ef3-4108-bbe1-e91db60288f0"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5) \n",
        "\n",
        "#logistic regression\n",
        "lr = LogisticRegression(solver='saga')\n",
        "res = build_cv_model(lr, train_x, y, kfold)\n",
        "\n",
        "print(\"Predicting on testing data\")\n",
        "lr.fit(train_x, y)\n",
        "pred_y = lr.predict(test_x)\n",
        "evaluate_models(test_y, pred_y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- cv 1\n",
            "f1_score 0.7832132011269255\n",
            "cohen_kappa_score: 0.6854483356633456\n",
            "--- cv 2\n",
            "f1_score 0.7793799302880206\n",
            "cohen_kappa_score: 0.6803466057094254\n",
            "--- cv 3\n",
            "f1_score 0.7852823315118398\n",
            "cohen_kappa_score: 0.6877516422970968\n",
            "--- cv 4\n",
            "f1_score 0.78580546336482\n",
            "cohen_kappa_score: 0.6886937470183931\n",
            "--- cv 5\n",
            "f1_score 0.7789589847342285\n",
            "cohen_kappa_score: 0.6801625179639126\n",
            "Average cv score, f1 0.782527982205167, cohen_kappa_score 0.6844805697304347\n",
            "Predicting on testing data\n",
            "f1_score 0.5194793536804309\n",
            "cohen_kappa_score: 0.4788738760899417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5194793536804309, 0.4788738760899417)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA50Xuk2yVNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "611599c0-8918-4a7d-86c3-3d4105c77398"
      },
      "source": [
        "#naive bayes\n",
        "nb = GaussianNB()\n",
        "res = build_cv_model(nb, train_x, y, kfold)\n",
        "\n",
        "print(\"\\n=====================\")\n",
        "print(\"Predicting on testing data\")\n",
        "nb.fit(train_x, y)\n",
        "pred_y = nb.predict(test_x)\n",
        "evaluate_models(test_y, pred_y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- cv 1\n",
            "f1_score 0.5802205749412404\n",
            "cohen_kappa_score: 0.38720461703587705\n",
            "--- cv 2\n",
            "f1_score 0.5688233370872234\n",
            "cohen_kappa_score: 0.37206595538312326\n",
            "--- cv 3\n",
            "f1_score 0.5814449383786909\n",
            "cohen_kappa_score: 0.388833183809172\n",
            "--- cv 4\n",
            "f1_score 0.5802877697841727\n",
            "cohen_kappa_score: 0.38565762224016287\n",
            "--- cv 5\n",
            "f1_score 0.5762369086312749\n",
            "cohen_kappa_score: 0.3809875501160582\n",
            "Average cv score, f1 0.5774027057645205, cohen_kappa_score 0.3829497857168787\n",
            "\n",
            "=====================\n",
            "Predicting on testing data\n",
            "f1_score 0.2563950558733734\n",
            "cohen_kappa_score: 0.18073046944050508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2563950558733734, 0.18073046944050508)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}