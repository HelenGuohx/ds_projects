{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import string\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, f1_score   # evaluation\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import en_core_web_sm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedKFold #交叉验证\n",
    "from sklearn.model_selection import GridSearchCV #网格搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy # tokenize text\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using small sample to do \n",
    "sample = dataset.sample(frac=0.2,random_state = 100) #261224 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    245369\n",
       "1    15856 \n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.target.value_counts()\n",
    "test_set.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #under_sampling to balance data\n",
    "def under_sampling(df,percent=1):\n",
    "    majority = df[df['target'] == 0]  \n",
    "    minority = df[df['target'] == 1]     \n",
    "    lower_data = majority.sample(n=int(percent*len(minority)),replace=False,random_state=890,axis=0)   \n",
    "    return (pd.concat([lower_data,minority]))\n",
    "\n",
    "\n",
    " #over sampling to balance data\n",
    "def over_sampling(df,percent=1):\n",
    "#通过numpy随机选取多数样本的采样下标\n",
    "    '''\n",
    "    percent:多数类别下采样的数量相对于少数类别样本数量的比例\n",
    "    '''\n",
    "    most_data = df[df['label'] == 1]  # 多数类别的样本\n",
    "    minority_data = df[df['label'] == 0]  # 少数类别的样本\n",
    "    index = np.random.randint(len(most_data), size=int(percent *len(minority_data)) )\n",
    "    #下采样后数据样本\n",
    "    lower_data = most_data.iloc[list(index)]  # 下采样\n",
    "    return(pd.concat([lower_data, minority_data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unicode space character with space ' '\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "def replace_space(text):\n",
    "    for s in spaces:\n",
    "        text = text.replace(s, ' ')\n",
    "    return text\n",
    "\n",
    "# clean rare words\n",
    "with open('rare_words.json') as f:\n",
    "    rare_words_mapping = json.load(f)\n",
    "    #print(rare_words_mapping)\n",
    "    \n",
    "def clean_rare_words(text):\n",
    "    for w in rare_words_mapping:\n",
    "        if text.count(w) > 0:\n",
    "            text = text.replace(w, rare_words_mapping[w])\n",
    "    return text\n",
    "    \n",
    "# decontracted\n",
    "def clean_decontracted(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
    "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
    "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
    "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
    "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
    "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
    "    return text\n",
    "\n",
    "# misspelling\n",
    "with open('misspell_words.json') as f:\n",
    "    misspell_words_mapping = json.load(f)\n",
    "def clean_misspell(text):\n",
    "    for w in misspell_words_mapping:\n",
    "        if text.count(w) > 0:\n",
    "            text = text.replace(w, misspell_words_mapping[w])\n",
    "    return text\n",
    "\n",
    "#replace punctuation with space\n",
    "def replace_punctuation(text):\n",
    "    punct = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(punct)\n",
    "\n",
    "# clean repeated letters\n",
    "def clean_repeat_words(text):\n",
    "    text = text.replace(\"img\", \"ing\")\n",
    "\n",
    "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
    "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
    "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
    "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
    "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
    "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
    "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
    "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
    "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
    "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
    "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
    "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
    "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
    "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
    "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
    "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
    "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
    "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
    "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
    "    text = re.sub(r\"plzz+\", \"please\", text)\n",
    "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
    "    return text\n",
    "# make text lower case\n",
    "def lower_words(text):\n",
    "    return text.lower()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    remove stop words and extra space\n",
    "    params: string\n",
    "    return: list\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words and w != ' ':\n",
    "            new_words.append(w)\n",
    "    return ' '.join(new_words)\n",
    "            \n",
    "def stemming(text):\n",
    "    pass\n",
    "\n",
    "#apply all the clean methods\n",
    "def text_cleaning(text):\n",
    "    text = replace_space(text)\n",
    "    text = clean_rare_words(text)\n",
    "    text = clean_decontracted(text)\n",
    "    text = clean_misspell(text)\n",
    "    text = replace_punctuation(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    text = lower_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data after cleaning\n",
    "# clean_text = sample.question_text.apply(lambda x: text_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like apples like hahahhhhh'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaning('i like apples, just like hahahhhhh?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "just\n"
     ]
    }
   ],
   "source": [
    "itext = 'i like apples, just like hahahhhhh?'\n",
    "for i in itext.split():\n",
    "    if i in stop_words:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras is on top of TensorFlow\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# tokenize text \n",
    "# embed_size = 300 #词向量维度\n",
    "# max_features = 95000 #设置词典大小\n",
    "# max_len = 70 #设置输入的长度\n",
    "\n",
    "# tokenizer = Tokenizer(num_words = max_features)\n",
    "# tokenizer.fit_on_texts(list(train_X))\n",
    "# train_X = tokenizer.texts_to_sequences(train_X)\n",
    "# test_X = tokenizer.texts_to_sequences(test_X)\n",
    " \n",
    "# # pad the sentences\n",
    "# train_X = pad_sequences(train_X, maxlen = max_len)\n",
    "# test_X = pad_sequences(test_X, maxlen = max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(typeToLoad):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')\n",
    "    \n",
    "    if typeToLoad==\"glove\":\n",
    "        file = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    elif typeToLoad==\"word2vec\":\n",
    "        #file = 'embeddings⁩/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin⁩'\n",
    "        file = 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)  # query word vector from the file\n",
    "    elif typeToLoad==\"fasttext\":\n",
    "        #file = \"⁨embeddings⁩/wiki-news-300d-1M⁩/wiki-news-300d-1M.vec\"\n",
    "        file = 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_maxtrix2(embeddings_index, word_index, max_features=100000, emb_size=300):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, emb_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(token_list, embeddings_index):\n",
    "    matrix = []\n",
    "    for tl in token_list:\n",
    "        vectors = []\n",
    "        for word in tl:\n",
    "            if word in embeddings_index:\n",
    "                vector = embeddings_index[word]\n",
    "                vectors.append(vector)\n",
    "        tl_vectors = np.mean(np.array(vectors), axis=0) if vectors else [0]*300\n",
    "        matrix.append(tl_vectors)\n",
    "    \n",
    "    return np.array(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text,embeddings_index):\n",
    "    text_list = text.split()\n",
    "    vectors = []\n",
    "    for word in text_list:\n",
    "        if word in embeddings_index:\n",
    "            vector = embeddings_index[word]\n",
    "            vectors.append(vector)\n",
    "    avg_vectors = np.mean(np.array(vectors), axis=0) if vectors else [0]*300\n",
    "    return avg_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oov_rate(text, embeddings_index):\n",
    "    text_list = text.split()\n",
    "    num_of_words = len(text_list)\n",
    "    num_of_known_words = 0\n",
    "    for word in text_list:\n",
    "        if word in embeddings_index:\n",
    "            num_of_known_words += 1\n",
    "    oov_rate = 1 - num_of_known_words / num_of_words if num_of_words else None\n",
    "    return oov_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# embed_glove = load_embed('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_word2vec = load_embed('word2vec')\n",
    "embed_fasttext = load_embed('fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "itext = 'Whyyyyyyyyy the h*ck do Brits end every sentence with \"x\"?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorize(itext,embed_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    num_known_words = 0\n",
    "    num_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            num_known_words += vocab[word]\n",
    "\n",
    "        else:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            num_unknown_words += vocab[word]\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(num_known_words / (num_known_words + num_unknown_words)))\n",
    "#     unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def vocab_check_coverage(texts, embed_method):\n",
    "\n",
    "    vocab = build_vocab(texts)\n",
    "    oov_words = check_coverage(vocab, embed_method)\n",
    "    oov = {\"oov_rate\": len(oov_words) / len(vocab), 'oov_words': oov_words}\n",
    "    print(\"oov_rate\", oov[\"oov_rate\"])\n",
    "    \n",
    "    return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate_features\n",
    "def concate_features(df):\n",
    "    feature_matrix = []\n",
    "    cnt = 0\n",
    "    for row in df.iterrows():\n",
    "        x = row[1]\n",
    "        new_vectors = x['word_vector']\n",
    "        new_vectors = np.append(new_vectors, x[\"oov_rate\"])\n",
    "        new_vectors = np.append(new_vectors, x[\"text_len\"])\n",
    "        new_vectors = np.append(new_vectors, x[\"clean_text_len\"])\n",
    "        feature_matrix.append(new_vectors)\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_features2(vectors, *args):\n",
    "    new_vectors = vectors + [*args]\n",
    "    return new_vectors\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_demension(X, n):\n",
    "    \"\"\"\n",
    "    X: features matrix\n",
    "    n: number of compoments or total explained ratio we want\n",
    "    return:\n",
    "    ev: explained variance of each component\n",
    "    evr: explained variance ratio of each component\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X)\n",
    "    ev = pca.explained_variance_\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    return ev, evr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = test_set.sample(frac=0.1,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set = under_sampling(train_set, percent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set['text_len'] = sample_train_set.question_text.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set[\"clean_text\"] = sample_train_set.question_text.apply(lambda x: text_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set['clean_text_len'] = sample_train_set.clean_text.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set[\"word_vector\"] = sample_train_set.clean_text.apply(lambda x:vectorize(x,embed_glove ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_set[\"oov_rate\"]  = sample_train_set.clean_text.apply(lambda x:compute_oov_rate(x, embed_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardlize\n",
    "# text_len_mean = np.mean(sample_train_set[\"text_len\"])\n",
    "# text_len_std = np.std(sample_train_set[\"text_len\"])\n",
    "# sample_train_set[\"text_len_standard\"] = sample_train_set['text_len'].apply(lambda x: x - text_len_mean/ text_len_std)\n",
    "# sample_train_set[\"clean_text_len_standard\"] = sample_train_set['clean_text_len'].apply(lambda x: x - np.mean(x)/ np.std(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train feature matrix\n",
    "train_matrix = concate_features(sample_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Word Embedding############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 64.96% of vocab\n",
      "Found embeddings for  97.83% of all text\n",
      "oov_rate 0.35042372535745453\n",
      "CPU times: user 1.71 s, sys: 1.15 s, total: 2.86 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#word coverage\n",
    "oov_words_glove = vocab_check_coverage(sample_train_set.clean_text, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 45.24% of vocab\n",
      "Found embeddings for  93.52% of all text\n",
      "oov_rate 0.5476130274136848\n",
      "CPU times: user 1.77 s, sys: 3.78 s, total: 5.55 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov_words_word2vec = vocab_check_coverage(sample_train_set.clean_text, embed_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.48% of vocab\n",
      "Found embeddings for  96.51% of all text\n",
      "oov_rate 0.46517621917897606\n",
      "CPU times: user 1.73 s, sys: 150 ms, total: 1.88 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oov_words_fasttext = vocab_check_coverage(sample_train_set.clean_text, embed_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# test data preprocess\n",
    "test_set['text_len'] = test_set.question_text.apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "test_set[\"clean_text\"] = test_set.question_text.apply(lambda x: text_cleaning(x))\n",
    "test_set['clean_text_len'] = test_set.clean_text.apply(lambda x: len(x.split()))\n",
    "test_set[\"oov_rate\"]  = test_set.clean_text.apply(lambda x:compute_oov_rate(x, embed_glove))\n",
    "test_set[\"word_vector\"] = test_set.clean_text.apply(lambda x:vectorize(x, embed_glove))\n",
    "test_matrix = concate_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261225, 303)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5224,)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix[10][301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261225,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(y_true, y_pred):\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print('confusion_matrix(0,1):')\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print('cohen_kappa_score:', cohen_kappa_score(y_test,y_pred))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "kflod = StratifiedKFold(n_splits=10, shuffle = True,random_state=7)#将训练/测试数据集划分10个互斥子集，\n",
    "\n",
    "def find_best_model(model, param_grid, X_train, Y_train, X_test, Y_test):\n",
    "    grid_search = GridSearchCV(model,param_grid,scoring = 'f1',n_jobs = -1,cv = kflod)\n",
    "    #scoring指定损失函数类型，n_jobs指定全部cpu跑，cv指定交叉验证\n",
    "    grid_search.fit(X_train, Y_train) #运行网格搜索\n",
    "    print(grid_search.best_params_)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.cv_results_)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    evaluate_models(Y_test, y_pred)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(model, X_train, Y_train, X_test, Y_test):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict(X_test)\n",
    "    evaluate_models(Y_test, Y_predict)\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(train_matrix)\n",
    "y_train = sample_train_set.target\n",
    "X_test = np.nan_to_num(test_matrix)\n",
    "y_test = test_set.target\n",
    "# model = LogisticRegression(solver = 'saga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-b38308bb24d5>\u001b[0m in \u001b[0;36mdefine_model\u001b[0;34m(model, X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mY_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver = 'saga')\n",
    "define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97    245369\n",
      "           1       0.49      0.60      0.54     15856\n",
      "\n",
      "    accuracy                           0.94    261225\n",
      "   macro avg       0.73      0.78      0.75    261225\n",
      "weighted avg       0.94      0.94      0.94    261225\n",
      "\n",
      "confusion_matrix(0,1):\n",
      "[[235581   9788]\n",
      " [  6312   9544]]\n",
      "cohen_kappa_score: 0.5097613535232608\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = KNeighborsClassifier(n_neighbors=2)\n",
    "# define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SVC(kernel='linear', C=1)\n",
    "define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search \n",
    "# model = SVC()\n",
    "# param_grid = [\n",
    "# #     {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, \n",
    "#     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "#              ]\n",
    "# # find_best_model(model, param_grid, X_train, y_train, X_test, y_test)\n",
    "# grid_search = GridSearchCV(model,param_grid,scoring = 'f1',n_jobs = -1,cv = kflod)\n",
    "# #scoring指定损失函数类型，n_jobs指定全部cpu跑，cv指定交叉验证\n",
    "# grid_search.fit(X_train, y_train) #运行网格搜索\n",
    "# print(grid_search.best_params_)\n",
    "# print(grid_search.best_score_)\n",
    "# print(grid_search.cv_results_)\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "# evaluate_models(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86      4900\n",
      "           1       0.19      0.81      0.31       324\n",
      "\n",
      "    accuracy                           0.77      5224\n",
      "   macro avg       0.59      0.79      0.59      5224\n",
      "weighted avg       0.93      0.77      0.83      5224\n",
      "\n",
      "confusion_matrix(0,1):\n",
      "[[3777 1123]\n",
      " [  62  262]]\n",
      "cohen_kappa_score: 0.22911818384685267\n",
      "CPU times: user 348 ms, sys: 37 ms, total: 385 ms\n",
      "Wall time: 116 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = GaussianNB()\n",
    "define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92      4900\n",
      "           1       0.21      0.44      0.28       324\n",
      "\n",
      "    accuracy                           0.86      5224\n",
      "   macro avg       0.58      0.66      0.60      5224\n",
      "weighted avg       0.91      0.86      0.88      5224\n",
      "\n",
      "confusion_matrix(0,1):\n",
      "[[4351  549]\n",
      " [ 181  143]]\n",
      "cohen_kappa_score: 0.215190748704305\n",
      "CPU times: user 2.5 s, sys: 14.9 ms, total: 2.51 s\n",
      "Wall time: 2.53 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = DecisionTreeClassifier() #default=”gini”\n",
    "define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      4900\n",
      "           1       0.66      0.37      0.47       324\n",
      "\n",
      "    accuracy                           0.95      5224\n",
      "   macro avg       0.81      0.68      0.72      5224\n",
      "weighted avg       0.94      0.95      0.94      5224\n",
      "\n",
      "confusion_matrix(0,1):\n",
      "[[4840   60]\n",
      " [ 205  119]]\n",
      "cohen_kappa_score: 0.4488310612816919\n",
      "CPU times: user 7.88 s, sys: 27.3 ms, total: 7.91 s\n",
      "Wall time: 7.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "define_model(model,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_train_set[sample_train_set['oov_rate']<0.2]['oov_rate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "#sample_train_set['text_len']\n",
    "# x = sample_train_set[sample_train_set['oov_rate']<0.2]['oov_rate']\n",
    "mu = np.mean(x)\n",
    "sigma = np.std(x)\n",
    "\n",
    "num_bins = 50\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(x, num_bins, density=1, color='#fcb43e')\n",
    "\n",
    "# add a 'best fit' line\n",
    "# y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "#      np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "# ax.plot(bins)\n",
    "ax.set_xlabel('oov_rate')\n",
    "ax.set_ylabel('Probability density')\n",
    "# ax.set_title(r'')\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "fig.tight_layout()\n",
    "plt.savefig('oov_rate.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fcf9dfbd479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
